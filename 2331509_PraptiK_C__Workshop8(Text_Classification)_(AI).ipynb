{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7DGbuYtB1JI",
        "outputId": "5dba7aa7-79ba-49b7-a57b-476f56f47fb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/AI/Copy of trum_tweet_sentiment_analysis.csv')\n",
        "\n",
        "# Show first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ1b6h6kzxmq",
        "outputId": "7c724942-dc9e-4f72-d955-34cd4a547c01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  Sentiment\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...          0\n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...          0\n",
            "2  Trump protests: LGBTQ rally in New York https:...          1\n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...          0\n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlvS4bnvz7dM",
        "outputId": "22b9f63e-a335-4cd6-8fd7-29d011c92b8e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'Sentiment'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')       # Tokenizer\n",
        "nltk.download('stopwords')   # Stop words\n",
        "nltk.download('wordnet')     # Lemmatizer dictionary\n",
        "nltk.download('omw-1.4')     # Lemma support for WordNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBn2owswz-oR",
        "outputId": "3db0ee18-a61e-4f31-c2a7-499d3e3f475f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaning_pipeline(dataset, rule = \"lemmatize\"):\n",
        "  \"\"\"\n",
        "  This...\n",
        "  \"\"\"\n",
        "  # Convert the input to small/lower order.\n",
        "  data = dataset.lower()\n",
        "\n",
        "  # Remove URLs\n",
        "  data = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', data, flags=re.MULTILINE)\n",
        "\n",
        "  # Remove emojis\n",
        "  data = re.sub(r'[^\\x00-\\x7F]+', '', data)\n",
        "\n",
        "\n",
        "  # Remove all other unwanted characters.\n",
        "  data = re.sub(f\"[{re.escape(string.punctuation)}]\",\"\",data)\n",
        "  # Create tokens.\n",
        "  tokens = re.findall(r'\\b\\w+\\b', data)\n",
        "  # Remove stopwords:\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  if rule == \"lemmatize\":\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  elif rule == \"stem\":\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "  else:\n",
        "    print(\"Pick between lemmatize or stem\")\n",
        "\n",
        "\n",
        "  return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "FzBR6Wwk0F_V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnxVUA7d0WgH",
        "outputId": "b725bfdc-6a4f-49cf-e5b6-43925d3b7c80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'Sentiment'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['text'].apply(text_cleaning_pipeline)\n",
        "\n",
        "# Optional: preview cleaned text\n",
        "print(df[['text', 'clean_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNJoC3iL0aej",
        "outputId": "c68901e2-0ab0-490b-c2b9-7a44544d177c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  RT @JohnLeguizamo: #trump not draining swamp b...   \n",
            "1  ICYMI: Hackers Rig FM Radio Stations To Play A...   \n",
            "2  Trump protests: LGBTQ rally in New York https:...   \n",
            "3  \"Hi I'm Piers Morgan. David Beckham is awful b...   \n",
            "4  RT @GlennFranco68: Tech Firm Suing BuzzFeed fo...   \n",
            "\n",
            "                                          clean_text  \n",
            "0  rt johnleguizamo trump draining swamp taxpayer...  \n",
            "1  icymi hacker rig fm radio station play antitru...  \n",
            "2  trump protest lgbtq rally new york bbcworld vi...  \n",
            "3  hi im pier morgan david beckham awful donald t...  \n",
            "4  rt glennfranco68 tech firm suing buzzfeed publ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['Sentiment'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "jodjZqx90fKv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "xuyejJih0ie5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ZPRTYsBP0qQZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRvQlLW60q-D",
        "outputId": "9a41f6c4-3053-410a-d131-682afeea209e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96    248563\n",
            "           1       0.94      0.91      0.92    121462\n",
            "\n",
            "    accuracy                           0.95    370025\n",
            "   macro avg       0.95      0.94      0.94    370025\n",
            "weighted avg       0.95      0.95      0.95    370025\n",
            "\n"
          ]
        }
      ]
    }
  ]
}